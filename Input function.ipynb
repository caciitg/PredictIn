{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impressions=[]\n",
    "# engagement=[]\n",
    "# for i in range(data.shape[0]):\n",
    "#     imp=((data['followers'][i]*(1/50))+(data['reactions'][i]*30)+(data['comments'][i]*50))/(data['t_factor'][i])\n",
    "#     impressions.append(imp)\n",
    "#     eng=(data['reactions'][i]+(data['comments'][i]*2))/(imp)\n",
    "#     engagement.append(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv('da.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flair in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (0.7)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: deprecated>=1.2.4 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (1.2.11)\n",
      "Requirement already satisfied: janome in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (0.4.1)\n",
      "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (3.8.3)\n",
      "Requirement already satisfied: transformers<=3.5.1,>=3.5.0 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (3.5.1)\n",
      "Requirement already satisfied: ftfy in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (5.9)\n",
      "Requirement already satisfied: regex in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (2020.11.13)\n",
      "Requirement already satisfied: gdown in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (3.12.2)\n",
      "Requirement already satisfied: tabulate in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (0.8.9)\n",
      "Requirement already satisfied: segtok>=1.5.7 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (1.5.10)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (0.2.5)\n",
      "Requirement already satisfied: mpld3==0.3 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (2.8.1)\n",
      "Requirement already satisfied: sentencepiece<=0.1.91 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (0.1.91)\n",
      "\n",
      "Requirement already satisfied: bpemb>=0.3.2 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (0.3.2)\n",
      "Requirement already satisfied: langdetect in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (1.0.8)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (0.24.1)\n",
      "Requirement already satisfied: torch>=1.1.0 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (1.7.1)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (4.56.2)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (3.3.4)\n",
      "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (4.6.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (4.6.2)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from flair) (1.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from bpemb>=0.3.2->flair) (1.19.4)\n",
      "Requirement already satisfied: requests in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from bpemb>=0.3.2->flair) (2.25.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from gensim<=3.8.3,>=3.4.0->flair) (0.29.14)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.6.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from gensim<=3.8.3,>=3.4.0->flair) (4.2.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.15.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from hyperopt>=0.1.1->flair) (2.5)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from hyperopt>=0.1.1->flair) (1.6.0)\n",
      "Requirement already satisfied: future in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from hyperopt>=0.1.1->flair) (0.18.2)\n",
      "Requirement already satisfied: overrides==3.0.0 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from matplotlib>=2.2.3->flair) (8.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from networkx>=2.2->hyperopt>=0.1.1->flair) (4.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from torch>=1.1.0->flair) (3.7.4.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from transformers<=3.5.1,>=3.5.0->flair) (3.15.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from transformers<=3.5.1,>=3.5.0->flair) (20.8)\n",
      "Requirement already satisfied: tokenizers==0.9.3 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from transformers<=3.5.1,>=3.5.0->flair) (0.9.3)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from transformers<=3.5.1,>=3.5.0->flair) (0.0.43)\n",
      "Requirement already satisfied: filelock in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from transformers<=3.5.1,>=3.5.0->flair) (3.0.12)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (4.0.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
      "Requirement already satisfied: click in c:\\users\\shrey\\anaconda3\\envs\\webscrape\\lib\\site-packages (from sacremoses->transformers<=3.5.1,>=3.5.0->flair) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import SentenceTransformerDocumentEmbeddings\n",
    "from torch.nn import CosineSimilarity\n",
    "import torch\n",
    "\n",
    "cs = CosineSimilarity()\n",
    "emb=SentenceTransformerDocumentEmbeddings('stsb-roberta-base')\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "def pre_process(corpus):\n",
    "    # convert input corpus to lower case.\n",
    "    corpus = corpus.lower()\n",
    "    # collecting a list of stop words from nltk and punctuation form\n",
    "    # string class and create single array.\n",
    "    stopset = stopwords.words('english') + list(string.punctuation)\n",
    "    # remove stop words and punctuations from string.\n",
    "    corpus=remove_emoji(corpus)\n",
    "    # word_tokenize is used to tokenize the input corpus in word tokens.\n",
    "    corpus = \" \".join([i for i in word_tokenize(corpus) if i not in stopset])\n",
    "    # remove non-ascii characters\n",
    "    corpus = unidecode(corpus)\n",
    "    return corpus\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.6B/glove.6B.50d.txt'\n",
    "word2vec_output_file = 'word2vec.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "filename = 'word2vec.txt'\n",
    "word_emb_model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "def map_word_frequency(document):\n",
    "    return Counter(itertools.chain(*document))\n",
    "    \n",
    "def get_sif_feature_vectors(sentence1, sentence2, word_emb_model=word_emb_model):\n",
    "    sentence1 = [token for token in sentence1.split() if token in word_emb_model.wv.vocab]\n",
    "    sentence2 = [token for token in sentence2.split() if token in word_emb_model.wv.vocab]\n",
    "    word_counts = map_word_frequency((sentence1 + sentence2))\n",
    "    embedding_size = 50 # size of vectore in word embeddings\n",
    "    a = 0.001\n",
    "    sentence_set=[]\n",
    "    for sentence in [sentence1, sentence2]:\n",
    "        vs = np.zeros(embedding_size)\n",
    "        sentence_length = len(sentence)\n",
    "        for word in sentence:\n",
    "            a_value = a / (a + word_counts[word]) # smooth inverse frequency, SIF\n",
    "            vs = np.add(vs, np.multiply(a_value, word_emb_model.wv[word])) # vs += sif * word_vector\n",
    "        vs = np.divide(vs, sentence_length) # weighted average\n",
    "        sentence_set.append(vs)\n",
    "    return sentence_set\n",
    "\n",
    "def get_feature_vectors(sentence1, sentence2, word_emb_model=word_emb_model):\n",
    "    sentence1 = [token for token in sentence1.split() if token in word_emb_model.wv.vocab]\n",
    "    sentence2 = [token for token in sentence2.split() if token in word_emb_model.wv.vocab]\n",
    "    word_counts = map_word_frequency((sentence1 + sentence2))\n",
    "    embedding_size = 50 # size of vectore in word embeddings\n",
    "    \n",
    "    sentence_set=[]\n",
    "    for sentence in [sentence1, sentence2]:\n",
    "        vs = np.zeros(embedding_size)\n",
    "        sentence_length = len(sentence)\n",
    "        for word in sentence:\n",
    "            a_value = 1 # smooth inverse frequency, SIF\n",
    "            vs = np.add(vs, np.multiply(a_value, word_emb_model.wv[word])) # vs += sif * word_vector\n",
    "        vs = np.divide(vs, sentence_length) # weighted average\n",
    "        sentence_set.append(vs)\n",
    "    return sentence_set\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_cosine_similarity(feature_vec_1, feature_vec_2):    \n",
    "    return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_variables(post_content,about,headline):\n",
    "    text=post_content.lower()\n",
    "    \n",
    "    numb_hashtags=len([tag.strip(\"#\") for tag in text.split() if tag.startswith(\"#\")])\n",
    "    #classification\n",
    "    labels = {\"achievement\":[\"achievement\"],\"info\":[\"knowledge and facts\",\"updates and announcements\"],\"insights\":[\"insightful experiences and life lessons\"],\"job opening\":[\"recruiting\"],\"call to action\":[\"share with us in comments what are your opinions preferences and suggestions\"]}\n",
    "    label_emb = {}\n",
    "    for lab in labels:\n",
    "        label_emb[lab] = []\n",
    "        for labi in labels[lab]:\n",
    "            sen = Sentence(labi)\n",
    "            emb.embed(sen)\n",
    "            label_emb[lab].append(sen.embedding.reshape(1,-1))\n",
    "    x=post_content\n",
    "    sen = Sentence(x)\n",
    "    emb.embed(sen)\n",
    "    sen_emb = sen.embedding.reshape(1,-1)\n",
    "    \n",
    "    label_sims = {}\n",
    "    \n",
    "    for lab in label_emb:\n",
    "        simi = 0\n",
    "        for embd in label_emb[lab]:\n",
    "            simi+=cs(embd,sen_emb)\n",
    "        simi = simi/len(label_emb[lab])\n",
    "        label_sims[lab] = simi\n",
    "    \n",
    "    max_lab = \"other\"\n",
    "    max_sim = 0\n",
    "    \n",
    "    for lab in label_emb:\n",
    "        if(label_sims[lab]>max_sim):\n",
    "            max_sim = label_sims[lab].item()\n",
    "            max_lab = lab\n",
    "    post_type=max_lab  \n",
    "    confidence=max_sim   \n",
    "    #convert tweets to lower case\n",
    "    \n",
    "    \n",
    "#url removes\n",
    "    text=text.replace(r'(https|http)?:\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%|\\-)*\\b','')\n",
    "    text=text.replace(r'www\\.\\S+\\.com','')\n",
    "\n",
    "#removes retweets & cc\n",
    "    text=text.replace(r'rt|cc', '')\n",
    "\n",
    "#hashtags removes\n",
    "    text=text.replace(r'#', '')\n",
    "\n",
    "#user mention removes\n",
    "    text=text.replace(r'@\\S+', '')\n",
    "\n",
    "#emoji \n",
    "    text=text.replace(r'[^\\x00-\\x7F]+', '')\n",
    "\n",
    "#html tags\n",
    "    text=text.replace(r'<.*?>', '')\n",
    "\n",
    "#punctuation\n",
    "    text=text.replace('[{}]'.format(string.punctuation), '')\n",
    "\n",
    "#removes extra spaces\n",
    "    text=text.replace(r'( )+', ' ').strip()\n",
    "    \n",
    "    \n",
    "    \n",
    "#relevance score\n",
    "    info=about+headline\n",
    "    sentence_1=info\n",
    "    sentence_2=text\n",
    "   \n",
    "    sentence_1= pre_process(sentence_1)\n",
    "    sentence_2= pre_process(sentence_2)\n",
    "     \n",
    "    sentence_set=get_feature_vectors(sentence_1, sentence_2, word_emb_model=word_emb_model)\n",
    "    if np.isnan(sentence_set[0]).any():\n",
    "        similarity=0\n",
    "    elif np.isnan(sentence_set[1]).any():\n",
    "        similarity=0   \n",
    "\n",
    "    else:\n",
    "        similarity=get_cosine_similarity(sentence_set[0], sentence_set[1])\n",
    "    relevance_score=similarity\n",
    "    content_len=len(text)\n",
    "    \n",
    "    return numb_hashtags,content_len,relevance_score,post_type,confidence\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_content=df['content'][0]\n",
    "about=df['about'][0]\n",
    "headline=df['headline'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-69-85e07796607c>:81: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  sentence1 = [token for token in sentence1.split() if token in word_emb_model.wv.vocab]\n",
      "<ipython-input-69-85e07796607c>:82: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  sentence2 = [token for token in sentence2.split() if token in word_emb_model.wv.vocab]\n",
      "<ipython-input-69-85e07796607c>:92: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  vs = np.add(vs, np.multiply(a_value, word_emb_model.wv[word])) # vs += sif * word_vector\n"
     ]
    }
   ],
   "source": [
    "n,cl,rs,pt,cf=input_variables(post_content,about,headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7637785693193962"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'insights'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2860507667064667"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
